{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices Prediction (Top 16.5%)\n",
    "* **Exploratory Data Analysis**\n",
    "* **Feature Engineering**\n",
    "* **Machine Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview\n",
    "## 1.1 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "train.head()\n",
    "test.head()\n",
    "print('train data size : {} and test data size: {}'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Save the Id and Drop 'Id' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = train['Id']\n",
    "test_id = test['Id']\n",
    "train.drop(['Id'], axis = 1, inplace = True)\n",
    "test.drop(['Id'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine train and test data for features engineering\n",
    "ntrain = train.shape[0]\n",
    "y_train = train.SalePrice.values\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Transform some numerical features to categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSSubClass is an int64 type, but it represents the building class of the house. so the feature is categorical\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\n",
    "\n",
    "#MoSold, GarageYrBlt, YearBuilt, YearRemodAdd and YrSold are also categorical features with int64 type, but they are ordinal.\n",
    "#No need to change their dtypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = train.corr()\n",
    "plt.subplots(figsize=(24,18))\n",
    "sns.heatmap(corrmat.round(2), annot=True, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Correlations: Features VS Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = [feature for feature in all_data.columns if all_data[feature].dtype != 'object']\n",
    "cat_features = [feature for feature in all_data.columns if all_data[feature].dtype == 'object']\n",
    "\n",
    "print(len(num_features), len(cat_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=18, figsize=(12, 80))\n",
    "plt.subplots_adjust(right=1.5)\n",
    "cmap = sns.cubehelix_palette(dark=0.3, light=0.8, as_cmap=True)\n",
    "\n",
    "for i, feature in enumerate(num_features, 1):    \n",
    "    plt.subplot(18, 2, i)\n",
    "    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', size='SalePrice', palette=cmap, data=train)\n",
    "        \n",
    "    plt.xlabel('{}'.format(feature), size=15)\n",
    "    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n",
    "    \n",
    "    for j in range(2):\n",
    "        plt.tick_params(axis='x', labelsize=12)\n",
    "        plt.tick_params(axis='y', labelsize=12)\n",
    "    \n",
    "    plt.legend(loc='best', prop={'size': 8})\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Outliers\n",
    "all_data = all_data.drop(all_data[(all_data['GrLivArea']>4000) & (all_data['SalePrice']<300000)].index)\n",
    "all_data = all_data.drop(all_data[(all_data['1stFlrSF']>4000) & (all_data['SalePrice']<300000)].index)\n",
    "all_data = all_data.drop(all_data[(all_data['BsmtFinSF1']>4000) & (all_data['SalePrice']<300000)].index)\n",
    "all_data = all_data.drop(all_data[(all_data['LotFrontage']>250) & (all_data['SalePrice']<300000)].index)\n",
    "all_data = all_data.drop(all_data[(all_data['TotalBsmtSF']>6000) & (all_data['SalePrice']<300000)].index)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=22, figsize=(18, 120))\n",
    "plt.subplots_adjust(right=1.5, top=1.5)\n",
    "\n",
    "for i, feature in enumerate(cat_features, 1):    \n",
    "    plt.subplot(22, 2, i)\n",
    "    sns.swarmplot(x=feature, y='SalePrice', data=train, palette='Set3')\n",
    "        \n",
    "    plt.xlabel('{}'.format(feature), size=25)\n",
    "    plt.ylabel('SalePrice', size=25, labelpad=15)\n",
    "    \n",
    "    for j in range(2):\n",
    "        if train[feature].value_counts().shape[0] > 10:        \n",
    "            plt.tick_params(axis='x', labelsize=7)\n",
    "        else:\n",
    "            plt.tick_params(axis='x', labelsize=20)\n",
    "        plt.tick_params(axis='y', labelsize=20)\n",
    "            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Drop useless features**\n",
    "print(all_data.Utilities.value_counts())\n",
    "print(all_data.Street.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So it's safe to drop these two features\n",
    "all_data.drop(['Utilities','Street'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Processing \n",
    "## 2.1 analysis the Target Variable 'SalePrice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(all_data['SalePrice'].dropna() , fit=norm);\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(all_data['SalePrice'].dropna(), plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Transform this variable and make it more normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"SalePrice\"] = np.log1p(all_data[\"SalePrice\"])\n",
    "\n",
    "# re-plot previous figures\n",
    "sns.distplot(all_data['SalePrice'].dropna() , fit=norm)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(all_data['SalePrice'].dropna(), plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = all_data.SalePrice.dropna()\n",
    "all_data.drop(['SalePrice'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_na = (all_data.isnull().sum())\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "all_data_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the description text indicates, NA means 'No' for those features mentioned below\n",
    "all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n",
    "all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n",
    "all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ('Fence','FireplaceQu','GarageType', 'GarageFinish', 'GarageQual','MasVnrType','MSSubClass',\n",
    "            'GarageCond','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','MasVnrArea',\n",
    "            'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
    "    all_data[col] = all_data[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n",
    "all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for functional and electrical etc., using the most common values to fillna\n",
    "all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n",
    "all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n",
    "all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n",
    "all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\n",
    "all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n",
    "all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check again\n",
    "all_data_na = all_data.isnull().sum()\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "all_data_na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Label Encoding some categorical variables that may contain information in their ordering set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [feature for feature in all_data.columns if all_data[feature].dtype == 'object' and all_data[feature].nunique() <= 5]\n",
    "cat_features.append('MoSold')\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for c in cat_features:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all_data[c].values)) \n",
    "    all_data[c] = lbl.transform(list(all_data[c].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Skewed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = [feature for feature in all_data.columns if all_data[feature].dtype != 'object']\n",
    "skewed_features = all_data[num_features].apply(lambda x: skew(x)).sort_values(ascending = False)\n",
    "skewed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Cox Transformation of skewed features\n",
    "skewness = skewed_features[abs(skewed_features) > 0.75]\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "skewed_index = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_index:\n",
    "    all_data[feat] = boxcox1p(all_data[feat], lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Getting dummy categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.get_dummies(all_data)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data[:len(y_train)]\n",
    "test = all_data[len(y_train):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from mlxtend.regressor import StackingCVRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Cost Functions & Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_train, y_pred):\n",
    "     return np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "\n",
    "def cv_rmse(model, X=train, y=y_train):    \n",
    "    return np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=kf))\n",
    "\n",
    "K = 5    \n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Models & Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=np.arange(14.5, 15.6, 0.1), cv=kf))\n",
    "lasso = make_pipeline(RobustScaler(), LassoCV(alphas=np.arange(0.0001, 0.0009, 0.0001), random_state=35, cv=kf))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNetCV(alphas=np.arange(0.0001, 0.0008, 0.0001), l1_ratio=np.arange(0.8, 1, 0.025), cv=kf))\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(learning_rate=0.01,\n",
    "                            n_estimators=3500,max_depth=3,\n",
    "                            gamma=0.001,subsample=0.7,\n",
    "                            colsample_bytree=0.7,objective='reg:linear',\n",
    "                            nthread=-1,seed=41,reg_alpha=0.0001)\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "\n",
    "stack = StackingCVRegressor(regressors=(lasso, ENet, ridge,model_xgb, model_lgb), meta_regressor=model_xgb, use_features_in_secondary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'RidgeCV': ridge,\n",
    "          'LassoCV': lasso, \n",
    "          'ElasticNetCV': ENet, \n",
    "          'LightGBMRegressor': model_lgb, \n",
    "          'XGBoostRegressor': model_xgb, \n",
    "         'StackingCVRegressor': stack}\n",
    "scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    model.fit(train, y_train)\n",
    "    \n",
    "    score = cv_rmse(model, X=train, y=y_train)\n",
    "    scores[name] = (score.mean(), score.std())\n",
    "    \n",
    "    \n",
    "    print('{} Mean RMSE: {:.6f} / Std: {:.6f}\\n'.format(name, scores[name][0], scores[name][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_predict(X):\n",
    "    return ((0.25 * ENet.predict(X)) + \n",
    "            (0.25 * ridge.predict(X))+\n",
    "            (0.1 * lasso.predict(X)) +\n",
    "            (0.1 * model_xgb.predict(X)) +\n",
    "            (0.1 * model_lgb.predict(X)) +\n",
    "            (0.2 * stack.predict(X)))\n",
    "\n",
    "blended_score = rmse(y_train, blend_predict(train))\n",
    "print('Blended Prediction RMSE: {}'.format(blended_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(columns=['Id', 'SalePrice'])\n",
    "submission_df['Id'] = test_id\n",
    "submission_df['SalePrice'] = np.expm1(blend_predict(test))\n",
    "submission_df.to_csv('sub.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
